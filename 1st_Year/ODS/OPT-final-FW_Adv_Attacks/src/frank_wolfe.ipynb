{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def classic_fw(\n",
    "    objective_fun, \n",
    "    gradient_fun, \n",
    "    LMO, \n",
    "    x0, \n",
    "    hyperparams = {\"max_iterations\": 20, \"tolerance\": 1e-6}\n",
    "):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - objective_fun (callable): Function f(x) to minimize (or maximize if sign=-1).\n",
    "        - gradient_fun (callable): Gradient of f(x).\n",
    "        - projection_operator (callable): Projection onto the feasible set (e.g., Lâˆž-ball).\n",
    "        - x0 (np.ndarray): Initial feasible point.\n",
    "        - hyperparams (dict): Dictionary with hyperparameters:\n",
    "            - \"max_iterations\" (int): Maximum number of iterations.\n",
    "            - \"tolerance\" (float): Tolerance on gradient norm.\n",
    "    Outputs:\n",
    "        - x_t (np.ndarray): Final solution.\n",
    "        - t (int): Number of iterations performed.\n",
    "        - history (dict): Contains 'objective' and 'gradient_norm'.\n",
    "    \"\"\"\n",
    "    # Defining the parameters\n",
    "    x_t = x0.copy().astype(np.float64)\n",
    "    max_iterations = hyperparams[\"max_iterations\"]\n",
    "    tolerance = hyperparams[\"tolerance\"]\n",
    "\n",
    "    # History trackers\n",
    "    history = {'objective': [], 'gap': [], 'gradient': []}\n",
    "\n",
    "    # Starting the Frank-Wolfe iterations\n",
    "    for t in range(1, max_iterations + 1):\n",
    "        \n",
    "        # Compute the gradient at the current point\n",
    "        grad_t = gradient_fun(x_t)\n",
    "\n",
    "        # Compute the FW direction\n",
    "        s_t = LMO(grad_t)\n",
    "        d_t = s_t - x_t\n",
    "        \n",
    "        # Compute the duality gap\n",
    "        gap = -grad_t @ d_t\n",
    "        \n",
    "        # Store history\n",
    "        history[\"gradient\"].append(grad_t)\n",
    "        history[\"objective\"].append(objective_fun(x_t))\n",
    "        history[\"gap\"].append(gap)\n",
    "\n",
    "        # Check for convergence\n",
    "        if gap < tolerance:\n",
    "            print(f\"Duality gap below tolerance at iteration {t}: {gap:.2e}\")\n",
    "            break\n",
    "\n",
    "        # Compute the step size\n",
    "        gamma_t = 2.0 / (t + 2.0)\n",
    "\n",
    "        # Compute the next step\n",
    "        x_t += gamma_t * d_t\n",
    "        \n",
    "    return x_t, t, history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
